# yA0tzy Configuration - Local CPU
# This config is validated by both Rust (serde) and Python (pydantic).
# See documentation/prd.md for detailed knob descriptions.

# Inference server settings
inference:
  # Bind address: "unix:///path/to/sock" or "tcp://host:port"
  bind: "unix:///tmp/yatzy_infer.sock"
  # Device: "cpu" or "cuda"
  device: "cpu"
  # Maximum batch size before flushing inference requests
  # Default: 32 (better tail latency on CPU than 64 in our measurements).
  max_batch: 32
  # Maximum wait time (microseconds) before flushing a partial batch
  # Default 1000us: keeps good throughput at low/medium load while still keeping latency reasonable.
  # Note: observed `H_wait.age_us` includes queue/backlog time, not just this batching window.
  max_wait_us: 1000
  # Metrics/control HTTP server bind address (used for hot-reload and Prometheus metrics)
  metrics_bind: "127.0.0.1:18080"

# Controller settings (orchestration)
controller:
  # Number of full iteration cycles to run (selfplay → train → gate)
  # Set higher for multi-iteration training runs
  total_iterations: 10

# Replay buffer settings
replay:
  # Keep at most N shards under runs/<id>/replay/ (prune older beyond capacity)
  # Prevents unbounded disk growth during long runs
  # Comment out or set to 0 to disable pruning (keep all shards)
  capacity_shards: 20

# MCTS (Monte Carlo Tree Search) settings
mcts:
  # PUCT exploration constant
  c_puct: 1.5
  # Simulation budget for reroll decisions
  budget_reroll: 400
  # Simulation budget for mark (category selection) decisions
  budget_mark: 400
  # Maximum in-flight inference requests per game
  max_inflight_per_game: 8
  # Dirichlet noise alpha (self-play only, not used in gating)
  dirichlet_alpha: 0.3
  # Dirichlet noise epsilon - fraction of noise mixed into priors (self-play only)
  dirichlet_epsilon: 0.25

# Self-play settings
selfplay:
  # Number of games to generate per training iteration
  games_per_iteration: 100
  # Number of worker processes
  workers: 4
  # Number of threads per worker process
  threads_per_worker: 2

# Training settings
training:
  # Mini-batch size for gradient updates
  batch_size: 256
  # Learning rate
  learning_rate: 0.0001
  # Whether to train candidate continuously across iterations (even on gate rejects)
  continuous_candidate_training: true
  # Weight decay (L2 regularization) - 0 disables
  weight_decay: 0.0001
  # Number of epochs per training iteration
  epochs: 10

# Gating (candidate vs best evaluation) settings
gating:
  # Number of games to play for evaluation
  games: 100
  # Base seed for deterministic paired-seed scheduling (used if seed_set_id not set)
  seed: 0
  # Fixed seed set id under configs/seed_sets/<id>.txt (overrides `seed` for scheduling)
  seed_set_id: "dev_v2"
  # Win rate threshold for promotion (0.55 = 55%)
  win_rate_threshold: 0.55
  # Use paired seeds with side swap for reduced variance
  paired_seed_swap: true
  # Use deterministic event-keyed chance stream for reproducible evals
  deterministic_chance: true

  # Optional: fixed-set oracle diagnostics (async; disabled by default)
  fixed_oracle:
    enabled: false
    set_id: fixed_balanced_v2
    n: null

# Neural network model architecture settings
model:
  # Hidden layer size for the neural network
  hidden_dim: 256
  # Number of residual blocks in the network
  num_blocks: 2
